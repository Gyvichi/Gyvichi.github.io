---
title: 'Beautiful insights for Lambda Optimization'
date: 2025-02-29
permalink: /posts/2025/lambda/
tags:
  - CS
  - Mathematics
---


# 1. CHT and Lambda optimization

 DP 최적화가 궁금했었는데 내가 가지고 있는 알고리즘 입문서에는 관련 내용이 아예 언급이 없어서 직접 조사하게 되었다. 이후 많지는 않지만 몇몇 문제들을 풀어보고 코포 블로그를 참고하면서 배운 것들을 잘 모아서 소개해보려고 한다.

DP 최적화 알고리즘도 Knuth-Yao, CHT, Hierschburg 등 여러 트릭들이 있는데 다 복잡해서... 내가 이해한 알고리즘들에 대해서만 설명해보고자 한다. CHT와 lambda optimization을 설명하려 하는데, 모두 $dp$ 배열의 convexity와 직간접적으로 연관되어있다.

### 1.1. Convex Hull Trick

아래와 같은 형태의 dp를 생각해보자. 이때 $b$는 감소수열이다.

$$dp[i]={min}_{j<i} (a[i]b[j]+dp[j])$$

점화식에 아래와 같이 대입될 것이다.

- $dp[1]=a[1]b[0]+dp[0]$
- $dp[2] = min~(a[2]b[0]+dp[0],~a[2]b[1]+dp[1])$
- $dp[3] = min~ (a[3]b[0]+dp[0],~a[3]b[1]+dp[1],~a[3]b[2]+dp[2])$
- $dp[4] = min~ (a[4]b[0]+dp[0],~a[4]b[1]+dp[1],~a[4]b[2]+dp[2],~a[4]b[3]+dp[3])$

주 아이디어는 일차함수 $f(x)_j=x \cdot b[j] +dp[j]$를 두는 것이다. $x \in \mathbb{R}$로 생각하고, 따라서 우리가 구하고자 하는 **$dp[n]$은 $f_0(a[n]), f_1(a[n]), f_2(a[n]), \cdots$ 중 최솟값과 같다고 생각할 수 있다.**

이 일차함수들의 모음은 기울기가 감소수열 $b$이므로 그래프에서 아래와 같이 나타날 것이다.

![](https://i.imgur.com/S8Yeug4.png)

여기서 $dp$가 어떻게 갱신될까? $dp[i]$을 구할 때 $f_1(a[i]), f_2(a[i]), \cdots$를 모두 구하고 최솟값을 잡으면 되므로, 노란 선들의 그래프에서 $x=a[i]$ 선을 그었을 때 가장 작은 값이 $dp[i]$가 되는 것이다. 따라서 답 $dp[i]$는 초록선 중에 존재할 것이며 그 기울기가 감소하므로 항상 convex하다.

CHT는 교차점을 기준으로 이 선분들을 관리하여 $x=a[n]$에 해당하는 직선을 찾고 대입함으로써 이뤄진다. 



원래 $dp[n]$을 구할 때 $i, j$가 모두 $n$까지 연산되므로 $\mathcal{O}(n^2)$인데 CHT를 적용하면 $\mathcal{O}(nlogn)$으로 단축된다. 

### 1.2. $\lambda$ optimization

*이 글을 봐도 감이 안 잡힌다면 `peltorator`의 `8-hour Long Stream In Support Of Ukraine`이라는 제목의 유튜브 강의를 찾아보면 좋을 거 같다.*

이 트릭은 별칭이 많다 : discrete Lagrange method, Lagrangian relaxation, WQS (Qingshi Wang) binary search, aliens trick, etc. 이 트릭이 필요한 문제는 2012년 ptz camp나 중국 로컬 대회에도 소개되었다는데, IOI 2016년 마지막 문제 `Aliens`의 정해로 알려지며 웰노운이 됐다고 한다. `peltorator`에 따르면 1992년에 관련 페이퍼나 문서가 있었다는 거 같은데 못 찾았다. 아시는 분 계신다면 댓글이라도,,

이 트릭의 아이디어를 생각하는데 있어서 가장 고려하기 편한 예시는 아래 문제가 있다. 

>모두 0보다 크거나 같은 성분의 배열 $A[n]$을 $k$개 파티션으로 나눌 때 각 파티션의 score는 파티션 내 성분의 합의 제곱으로 정의한다. 
>파티션 개수 $k$, 배열 $A$가 주어질 때 score 총합의 최솟값을 출력하시오.

일반적으로 이런 류의 문제는 dp로 풀린다. $dp[i][c]$를 첫 $i$개 원소를 $c$개 파티션으로 나눌 때 최솟값이라 하자. 그러면 아래와 같은 dp 식을 세울 수 있다.

$$dp[i][k]=min_{j<i}~dp[j][k-1]+(a_{j+1}+\cdots+a_i)^2$$

이제 여러 최적화를 진행해야 하는데, 보통 prefix sum, cht를 진행한다. $\lambda$-optim이 여기서 적용 가능한데, **파티션 개수 $c$를 무시하고 단순 $dp[n]$을 생각하는 것**에서 시작된다. 이렇게 되면 항상 파티션이 $n$개일 것이다. ($a_1^2+a_2^2+\cdots \leq (a_1+a_2+\cdots)^2)$ 따라서 우리는 상수 $\lambda$를 파티션을 만들 때마다 score에 더하여 벌을 줄 거다.

$$dp[i]=\lambda + min_{j<i}~dp[j]+(S_i-S_j)^2$$

그러면 $\lambda$가 어떤 값이냐에 따라 이 식이 최적으로 생각하는 파티션 개수가 달라질 것이다. 우리는 이때 최적 파티션 개수를 $k$라 생각하게 하는 절묘한 $\lambda$를 찾을 것이고, 그때 위 $dp$를 풂으로써 답을 해결할 수 있다! 

아래 그래프 ($x$축 : $k$, $y$축 : $dp[n]$)에서 초록 선은 $\lambda$를 도입하지 않았을 때이고 파랑선은 $\lambda$를 도입했을 때이다. 그래프가 아래와 같이 그려진 이유는 초록선이 convex하고 파랑선은 초록선에 $k\lambda$가 더해진 형태이기 때문이다. (convex하다는 증명은 아래에서) 만약 $\lambda$를 주지 않으면 파티션을 나눌수록 $(k \rightarrow n)$ 계속 score가 낮아지지만 파티션을 줄수록 $k\lambda$를 주면 특정 부분에서 극솟값이 발생한다. $\lambda$에 따라 이 극솟값이 발생하는 지역이 변할 것이며, 

![](https://i.imgur.com/biNphFb.png)




# 2. $\lambda$의 존재성과 유일성을 위한 조건

이런 의문이 든다.
1. $\lambda$는 존재하는가?
2. $\lambda \in \mathbb{Z}$?
3. 

차근차근 알아보자.

### 2.1. Unimodality, Convexity, 





다양한 convex 정의에 대해 알아보자.

$$f(c+2)-f(c+1)~\leq,~\geq~ f(c+1)-f(c)$$
$$f(\lambda x_1+(1-\lambda)x_2) \leq \lambda f(x_1)+(1-\lambda)f(x_2),~\lambda \in [0, 1]$$
등호가 성립하면 non-strictly convex, 항상 $<$이면 strictly convex라 한다.

그러면 위 alien trick에서 발생하는 문제를 아래와 같이 쓸 수 있다.

>$f$가 non-strictly convex하면 항상 optimal한 $\lambda$가 이분탐색으로 특정되지 않는다.





Quadrangle inequality
$$W(a, c)+W(b, d) \leq W(a, d)+W(b, d)$$

>Cost 함수가 quadrangle inequality를 만족하면 $f$ 는 convex다.





# 3. 사실 $\lambda$는 Lagrange multiplier다

위 과정은 Lagrange multiplier와 관련이 깊다.

- $X$ : 배열의 가능한 모든 파티션들의 집합
- $f$ : 최소 score
- $h$ : 파티션 개수, 즉 $h(x)=k$





# 3. 사실 $\lambda$를 탐색하지 않아도 된다



# 4. Lambda optimiziation as dimensionality reducer?


#### 2.1. 라그랑주 승수법(Lagrange Multiplier)
라그랑주 승수법은 제약이 있는 함수의 최적화 알고리즘이다. 라그랑주 승수법의 메인 아이디어는 **두 함수가 접할 때 접선을 공유하여 gradient가 평행**하다는 것이다.
따라서 최적화하고자 하는 함수 $f$와 제약 $g$에 대해 $\nabla f = \lambda \nabla g$라 표현할 수 있다. (이때 $\lambda$를 Lagrange Multiplier라 부른다.)

예를 들어 $x^2+y^2=1$을 만족하는 $x, y \in \mathbb{R}$에 대해 $x+y$의 최대$\cdot$최소를 구할 때 라그랑주 승수를 적용하면 아래와 같다.
$$(2x, 2y) = \lambda (1, 1)$$
즉 $x=y=\frac{\lambda}{2}$이므로 이를 대입해서 풀어낼 수 있다.

한편 이 과정은 다변수함수 $f=c$에 대해 아래와 동일하다. (마지막 $g=c$가 대입 과정)
$$
\nabla f = \lambda \nabla g \rightarrow \nabla f - \lambda \nabla g = 0 \rightarrow
\begin{cases}
\frac{\partial f}{\partial x_1}(x_1, \ldots, x_n) - \lambda \frac{\partial g}{\partial x_1}(x_1, \ldots, x_n) = 0 \\
\frac{\partial f}{\partial x_2}(x_1, \ldots, x_n) - \lambda \frac{\partial g}{\partial x_2}(x_1, \ldots, x_n) = 0 \\
\vdots \\
\frac{\partial f}{\partial x_n}(x_1, \ldots, x_n) - \lambda \frac{\partial g}{\partial x_n}(x_1, \ldots, x_n) = 0
\end{cases}, \quad g(x_1, \ldots, x_n) = c
$$
마지막 연산이 일반화하기 까다로운데, 이는 **Lagrange function** $\mathcal{L}\equiv f+\lambda(c-g)$을 정의하여 $\nabla \mathcal{L}=0$을 계산함으로써 해결된다. $\mathcal{L}$을 $\lambda$로 편미분할 때 $c=g$가 도출되기 때문이다. 이렇게 함으로써 제약 조건이 사라진 하나의 함수로 만들 수 있다. 그리고 제약이 $g_1 \cdots g_k$일 때 그에 해당하는 $\lambda_i$를 정의함으로써 $\lambda$를 벡터로 일반화할 수 있다.

>$\therefore$ 제약 $g$가 걸린 함수 $f$의 최적화는 아래 수식을 통해 가능하다.
>$\nabla \mathcal{L} = \nabla f - \nabla \lambda (c-g) = 0$

#### 2.2. IOI 16' D2 Q6 Aliens
![](https://i.imgur.com/zAOa4ep.png)
문제는 아래와 같다.
>$m \times m$의 정사각형 행렬 위 $n$개의 중복 가능 점들이 $r[i],~c[i]$로 주어진다. $k$개의 **대각선이 주대각선과 일치하는 정사각형**을 그려 모든 점들을 포함시킬 때, 그린 정사각형들의 합집합이 차지하는 영역을 최소화하고자 한다. 이때 그 영역을 출력하시오. ($1 \times 1$의 넓이를 1로 두자.)
>
>**시간 제한 2초, $1 \leq n \leq 5e5,~1 \leq m \leq 1e6$

이 문제는 본래 점화식을 구성하여 Brute forcing보다 빠르게 푸는 다이나믹 프로그래밍 문제이다. $i$개 점을 포함하는 $k$개 정사각형이 가지는 최소 영역을 $a[i][k]$로 두자. 


이 문제의 중요한 점은 바로 주어지는 입력의 크기가 너무 커 시간 제한 내에 푸는 알고리즘을 작성하려면 많은 최적화가 필요하다는 것이다.

#### 2.3. 용어
- Penalty Method
	- 위 Lagrange Multiplier처럼 제약 문제를 제약되지 않은 문제로 풀어내는 기법을 모아 Penalty method라 한다. 제약되지 않은 함수로 풀어졌을 때 그 함수를 Penalty function이라 하는데, 이를 구성하는 parameter를 Penalty parameter라 한다.
- Epigraph $epi~f$
	- Epi-는 above를 뜻하여, 함수 $f$ 윗부분을 의미한다.



람다 최적화의 아이디어는 **다변수 점화식 중 제약에 해당하는 변수를 penaly parameter화하여 차원 하나를 죽인 penalty function을 만들어 제약을 완화시키는 것**이다.

$i$번째 점까지를 포함하는 영역 값을 수열 $a_{i, \cdots}$로 두자. 
여기서 아이디어는 $k$가 커질수록 비용 $\lambda$가 든다고 생각하는 것이다. 만약 $k=1$이면 한 정사각형이 모든 점을 포함해야 하므로 답이 최대일 것인데, $k=2,~3,~\cdots$이 될수록 cost $\lambda$가 들어 점점 영역이 감소하는 것이다. 


$k$를 무시하자. $i$째 점까지 덮는 최소 영역을 탐색하고자 한다. 이때 답에 해당하는 정사각형 개수는 무엇이 될 지 모른다. 그래서 $k$가 변수로 들어가야 하는 것인데, 람다 최적화의 아이디어는 정사각형으로 나눌 때마다 Cost $\lambda$를 부여한다는 것이다. 만약 $\lambda$가 0이라면 정사각형을 계속 사용해도 Cost가 없으므로 답은 모든 점을 위한 최소한의 정사각형을 사용하는 경우일 것이고, $\lambda$로 매우 큰 값을 사용한다면 한 번 새 정사각형을 사용할 때마다 Cost가 커져 큰 정사각형 하나만 사용될 것이다.

따라서, 모든 $\lambda$에 대해 탐색해가며 그 $\lambda$에서 $k$로 덮는 것이 최선인 경우를 찾는 것이다. 

한편 다시 생각해보면 실제 답에 매번 $\lambda$가 더해졌으므로, $a_i-k \lambda$가 답임을 알 수 있다.

필요한 것인데
이를 참고하여 복잡한 귀납적 관계를 $f$로 두고 $\lambda$를 추가하여 점화식을 세워보자.
$$a[i]=min~\{f(a[i-1])\}+\lambda$$

즉 이전 $i-1$개 점을 덮는 영역에서의 답에서 

#### 3.1. 직관적 접근





의 접근 이전에 $k$에 따른 답 함수를 생각해보자. 직관적으로 $k$가 늘어날수록 답 영역은 점점 줄어들며, 볼록할 것이라 추측할 수 있다. 우리는 $f(k)$를 구하면 된다.
![](https://i.imgur.com/iSYzGf2.png)
*$x$축이 $k$, $y$축이 답일 때 위와 같은 그래프 개형이 나타날 것이라 추측할 수 있다.* 

여기에 라그랑주 승수의 아이디어를 적용하여, 

즉 $k$가 커질수록 
이때 어떤 실수 $\lambda$에 대해 각 $k=k_i$마다의 접점을 구하는 건 어떨까?

$f(x)+\lambda x$이 최소화되는 지점 
여기서 Lagrange Multiplier의 아이디어를 따라 입력에 따라 결정되는 접선 $cx$가 있으면 $f(x)+cx$를 최소화하여 답을 구할 수 있다. 접선의 기울기는 $f(x)$의 볼록성에 의해 계속 감소하므로 이분탐색을 통해 $c$ 탐색을 가속할 수 있다. 

그래서 놀랍게도 시간복잡도 항은 $k$가 사라진 $\mathcal{O}(n log m)$이다!


$X$: 가능한 정사각형 $k$개들의 집합
$f(x)$를 $g(x)=y$ 제약에서 최소화해야 한다.

따라서 이에 대한 Lagrange dual function은 아래와 같다.
$$t(\lambda)=inf_{y \in Y}[h(y)-\lambda \cdot y]$$
#### 3.3. $\lambda$의 존재성과 정수가 아닐 가능성
$k$개 정사각형을 사용할 때 optimal $\lambda$값을 $f(c)$라 정의하자. 

$$f(i)=min_{t<i}~\{g(t)\}+(r_i-l_{i+1}+1)^2-max(r_t-l_{t+1}+1, 0)+\lambda$$




#### 4. 제언 및 소감

#### 5. Ref
- https://www.youtube.com/watch?v=WZKOdorb1Dg
- https://convex-optimization-for-all.github.io/
### 2.1. Monge Array
$1\leq a < b \leq n,~1 \leq c < d \leq m$인 $a, b, c, d$에 대해 아래를 만족하면 $n \times m$ 행렬 $A$는 monge array이다.
$A[a, c]+A[b, d] \leq A[a, d]+A[b, c]$



코포 댓글로부터 알게 됐는데, LP에 등장하는 Lagrange sufficiency thm이라는 것에 기반하는 트릭이라고 한다. 

### 3.1. Lagrange sufficiency thm.

*Minimize $f(x)$ s.t. $g(x)=b$, $x \in X \in \mathbb{R}^m,~f:\mathbb{R}^m \rightarrow \mathbb{R}$와 같은 최적화 문제*를 보면 $\lambda \in \mathbb{R}^n$에 대한 Lagrangian을 최적화하는 것으로 우회한다. 이에 대해 아래가 성립한다는 것이 Lagrange sufficiency thm이다.

>If $x^*$ and $\lambda^*$ exist s.t. $x^*$ is feasible and
>$$L(x^*, \lambda^*) \leq L(x, \lambda^*)~~~~\forall x \in X$$
 then $x^*$ is optimal.

$\lambda=\lambda^*$로 동일하니 그냥 $L=f, ~h(x)=b$이므로 자명하다. 


---

https://codeforces.com/blog/entry/98334
![](https://i.imgur.com/a6kcFwU.png)
https://mamnoonsiam.github.io/posts/attack-on-aliens.html

https://www.yohandi.me/blog/lagrange-relaxation/

https://www.youtube.com/watch?v=WZKOdorb1Dg

https://koosaga.com/243
